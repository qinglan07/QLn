#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
RF + SMOTE(k_neighbors=1) + StratifiedKFold + Optuna
åŒæ—¶è¾“å‡ºï¼š
 - RF å†…å»º feature_importances_ çš„å‰150ç‰¹å¾
 - Permutation Importance çš„å‰150ç‰¹å¾

è¯´æ˜ï¼š
 - ä½¿ç”¨ DataFrame æ ¼å¼ä¿ç•™ç‰¹å¾å
 - åœ¨ Optuna CV ä¸­å¯¹ train éƒ¨åˆ†ä½¿ç”¨ SMOTEï¼ˆé¿å…æ³„æ¼ï¼‰
 - æœ€ç»ˆè®­ç»ƒï¼šç”¨ SMOTE å¯¹å…¨é‡ 217 æ ·æœ¬è®­ç»ƒç”¨äºé¢„æµ‹
 - å¦å¤–è®­ç»ƒä¸€ä¸ªä¸å« SMOTE çš„ RFï¼ˆåœ¨åŒæ · scaler ä¸‹ï¼‰ç”¨äºè®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼ˆPermutation ä¸ å†…å»ºï¼‰ï¼Œé¿å… SMOTE å¯¹é‡è¦æ€§è®¡ç®—äº§ç”Ÿåå·®
"""
import os
import numpy as np
import pandas as pd
import optuna
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
from sklearn.pipeline import Pipeline
from sklearn.inspection import permutation_importance
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import joblib
import warnings
warnings.filterwarnings("ignore")

# -----------------------------
# é…ç½®è·¯å¾„ & è¯»å–æ•°æ®
# -----------------------------
CSV_PATH = "/home/oem/qinglan7/ICA/DATA/tt/DATA.CSV/ComPare_Vitselect.csv"
RESULT_DIR = "/home/oem/qinglan7/ICA/DATA/tt/DATA.CSV/RF_result/"

os.makedirs(RESULT_DIR, exist_ok=True)

df = pd.read_csv(CSV_PATH)

# ä½¿ç”¨ DataFrameï¼ˆä¿ç•™åˆ—åï¼‰
base_data = df.iloc[:272].reset_index(drop=True)       # ç”¨äºè®­ç»ƒ+CVï¼ˆ217æ¡ï¼‰
predict_data = df.iloc[272:339].reset_index(drop=True) # éœ€é¢„æµ‹çš„ 55 æ¡

X_base = base_data.drop(columns=["ID", "Class"])   # DataFrame ä¿ç•™åˆ—å
y_base = base_data["Class"].astype(int)

X_predict = predict_data.drop(columns=["ID", "Class"])
predict_ids = predict_data["ID"].values

feature_names = X_base.columns.to_numpy()

# -----------------------------
# KFold è®¾ç½®
# -----------------------------
N_SPLITS = 5
skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)

# -----------------------------
# Optuna ç›®æ ‡å‡½æ•°ï¼šSMOTE(k_neighbors=1) é˜²æ­¢æŠ¥é”™
# -----------------------------
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 200, 500),
        'max_depth': trial.suggest_int('max_depth', 5, 25),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),
    }

    rf = RandomForestClassifier(
        **params,
        class_weight="balanced",
        random_state=42,
        n_jobs=-1
    )

    # SMOTE è®¾ç½®ä¸º k_neighbors=1ï¼ˆå®‰å…¨ã€ä¸æŠ¥é”™ï¼‰
    smote = SMOTE(k_neighbors=1, random_state=42)

    # ä½¿ç”¨ imblearn çš„ Pipelineï¼ˆSMOTE åªåœ¨ fit_resample é˜¶æ®µç”Ÿæ•ˆï¼Œä¸ä¼šåœ¨ predict ä¸­æ›´æ”¹ï¼‰
    pipeline = ImbPipeline([
        ('scaler', StandardScaler()),
        ('smote', smote),
        ('rf', rf)
    ])

    cv_scores = []
    # åˆ†æŠ˜è®­ç»ƒä¸éªŒè¯ï¼ˆæ¯æŠ˜éƒ½åœ¨ train ä¸Šåš SMOTEï¼‰
    for train_idx, val_idx in skf.split(X_base, y_base):
        X_tr, X_val = X_base.iloc[train_idx], X_base.iloc[val_idx]
        y_tr, y_val = y_base.iloc[train_idx], y_base.iloc[val_idx]

        pipeline.fit(X_tr, y_tr)
        preds = pipeline.predict(X_val)
        cv_scores.append(f1_score(y_val, preds, average="macro"))

    # è¿”å›å¹³å‡ Macro-F1
    return float(np.mean(cv_scores))


# -----------------------------
# æ‰§è¡Œ Optuna æœç´¢
# -----------------------------
N_TRIALS = 120
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=N_TRIALS)

print("\nğŸ¯ Optuna æœç´¢ç»“æŸ")
print("Best params:", study.best_params)
print("Best CV Macro-F1:", study.best_value)

best_params = study.best_params

# -----------------------------
# æœ€ç»ˆè®­ç»ƒï¼šä¸¤ä¸ªæ¨¡å‹
# 1) final_pipeline: åŒ…å« SMOTE çš„å®Œæ•´ pipelineï¼ˆç”¨äºå¯¹ 55 æ¡è¿›è¡Œé¢„æµ‹ï¼‰
# 2) rf_no_smote_pipeline: æ—  SMOTEï¼Œä»… scaler + RFï¼ˆç”¨äºè®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼‰
# -----------------------------
# 1) å¸¦ SMOTE çš„æœ€ç»ˆ pipelineï¼ˆç”¨äºé¢„æµ‹ï¼‰
final_rf = RandomForestClassifier(
    **best_params,
    class_weight="balanced",
    random_state=42,
    n_jobs=-1
)
final_smote = SMOTE(k_neighbors=1, random_state=42)
final_pipeline = ImbPipeline([
    ('scaler', StandardScaler()),
    ('smote', final_smote),
    ('rf', final_rf)
])

final_pipeline.fit(X_base, y_base)  # åœ¨å…¨éƒ¨ 217 æ ·æœ¬ä¸Šç”¨ SMOTE è®­ç»ƒ

# 2) ä¸å« SMOTE çš„ pipelineï¼ˆç”¨äºç‰¹å¾é‡è¦æ€§è®¡ç®—ï¼‰
rf_no_smote = RandomForestClassifier(
    **best_params,
    class_weight="balanced",
    random_state=42,
    n_jobs=-1
)
rf_no_smote_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('rf', rf_no_smote)
])

rf_no_smote_pipeline.fit(X_base, y_base)  # åœ¨å…¨éƒ¨ 217 æ ·æœ¬ä¸Šè®­ç»ƒï¼ˆæ—  SMOTEï¼‰

# -----------------------------
# å¯¹ 55 æ¡æ ·æœ¬è¿›è¡Œé¢„æµ‹å¹¶ä¿å­˜
# -----------------------------
X_predict_df = X_predict  # ä¿æŒ DataFrame
preds = final_pipeline.predict(X_predict_df)
results_df = pd.DataFrame({
    "ID": predict_ids,
    "Predicted_Class": preds
})
pred_file = os.path.join(RESULT_DIR, "1RF_SMOTE_predictions_218-272.csv")
results_df.to_csv(pred_file, index=False)
print(f"\nâœ… é¢„æµ‹ç»“æœå·²ä¿å­˜: {pred_file}")

# -----------------------------
# ç‰¹å¾é‡è¦æ€§è®¡ç®—ä¸ä¿å­˜ï¼ˆå‰150ï¼‰
# ä¸¤ç§æ–¹å¼ï¼š
#  A) RF å†…å»º feature_importances_
#  B) Permutation Importanceï¼ˆåœ¨ rf_no_smote_pipeline ä¸Šè®¡ç®—ï¼‰
# -----------------------------
TOP_N = 150
if TOP_N > len(feature_names):
    TOP_N = len(feature_names)

# A) å†…å»º feature_importances_
rf_model_trained = rf_no_smote_pipeline.named_steps['rf']
builtin_importances = rf_model_trained.feature_importances_
order_builtin = np.argsort(builtin_importances)[::-1][:TOP_N]
builtin_df = pd.DataFrame({
    "Feature": feature_names[order_builtin],
    "Importance": builtin_importances[order_builtin]
})
builtin_file = os.path.join(RESULT_DIR, f"Top{TOP_N}_RF_builtin_feature_importances.csv")
builtin_df.to_csv(builtin_file, index=False)
print(f"âœ… RF å†…å»º feature_importances_ å·²ä¿å­˜: {builtin_file}")

# B) Permutation Importance
print("\nğŸ” æ­£åœ¨è®¡ç®— Permutation Importanceï¼ˆå¯èƒ½è€—æ—¶ï¼Œå–å†³äºç‰¹å¾æ•°é‡ä¸ repeatsï¼‰...")
perm_result = permutation_importance(
    rf_no_smote_pipeline,
    X_base,
    y_base,
    scoring="f1_macro",
    n_repeats=10,
    random_state=42,
    n_jobs=-1
)
perm_means = perm_result.importances_mean
order_perm = np.argsort(perm_means)[::-1][:TOP_N]
perm_df = pd.DataFrame({
    "Feature": feature_names[order_perm],
    "Importance": perm_means[order_perm]
})
perm_file = os.path.join(RESULT_DIR, f"Top{TOP_N}_PermutationImportance.csv")
perm_df.to_csv(perm_file, index=False)
print(f"âœ… Permutation Importance å‰ {TOP_N} å·²ä¿å­˜: {perm_file}")

# ä¸ºæ–¹ä¾¿å¯¹æ¯”ï¼Œä¹Ÿä¿å­˜å®Œæ•´ä¸¤ä¸ª importance çš„ CSVï¼ˆå…¨éƒ¨ç‰¹å¾ï¼‰
all_builtin_df = pd.DataFrame({"Feature": feature_names, "Importance": builtin_importances})
all_builtin_df = all_builtin_df.sort_values("Importance", ascending=False).reset_index(drop=True)
all_builtin_file = os.path.join(RESULT_DIR, f"All_RF_builtin_feature_importances.csv")
all_builtin_df.to_csv(all_builtin_file, index=False)

all_perm_df = pd.DataFrame({"Feature": feature_names, "Importance": perm_means})
all_perm_df = all_perm_df.sort_values("Importance", ascending=False).reset_index(drop=True)
all_perm_file = os.path.join(RESULT_DIR, f"All_PermutationImportance.csv")
all_perm_df.to_csv(all_perm_file, index=False)

# -----------------------------
# ä¿å­˜æ¨¡å‹ä¸ scaler
# -----------------------------
joblib.dump(final_pipeline, os.path.join(RESULT_DIR, "RF_SMOTE_pipeline.pkl"))
joblib.dump(rf_no_smote_pipeline, os.path.join(RESULT_DIR, "RF_no_SMOTE_pipeline_for_importance.pkl"))
print("\nâœ… æ¨¡å‹å·²ä¿å­˜ï¼š")
print(f"- å¸¦ SMOTE çš„é¢„æµ‹ pipeline: {os.path.join(RESULT_DIR, 'RF_SMOTE_pipeline.pkl')}")
print(f"- æ—  SMOTE çš„ importance pipeline: {os.path.join(RESULT_DIR, 'RF_no_SMOTE_pipeline_for_importance.pkl')}")

# -----------------------------
# å°ç»“è¾“å‡º
# -----------------------------
print("\n=== Summary ===")
print(f"- Optuna best params: {study.best_params}")
print(f"- Optuna best CV Macro-F1: {study.best_value:.4f}")
print(f"- Predictions file: {pred_file}")
print(f"- Top {TOP_N} RF built-in importance: {builtin_file}")
print(f"- Top {TOP_N} Permutation Importance: {perm_file}")
print(f"- All built-in importance: {all_builtin_file}")
print(f"- All permutation importance: {all_perm_file}")
print("\nFinished.")
