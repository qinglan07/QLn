#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
SELECT.pyï¼ˆæœ€ç»ˆä¼˜åŒ–ç‰ˆï¼‰
-------------------------------------------------------------
ä¸¤é˜¶æ®µç‰¹å¾é€‰æ‹©ï¼ˆt-test â†’ Mutual Infoï¼‰ + éšæœºæ£®æ—é¢„æµ‹
æ ¸å¿ƒé€»è¾‘ï¼š
1. t-test é˜¶æ®µï¼šä»…ä½¿ç”¨å‰217ä¸ªæ ·æœ¬è®¡ç®—ï¼Œç­›é€‰å·®å¼‚æ˜¾è‘—ç‰¹å¾
2. ç‰¹å¾è¿‡æ»¤ï¼šè‡ªåŠ¨å»é™¤æ–¹å·®ä¸º0çš„æ’å®šç‰¹å¾ï¼ˆé¿å…MIè¯¯é€‰ï¼‰
3. MI é˜¶æ®µï¼šç”¨å‰217ä¸ªæ ·æœ¬è®­ç»ƒï¼Œé€‰ä¸­é«˜å…³è”ç‰¹å¾
4. æ¨¡å‹è¯„ä¼°ï¼šè®­ç»ƒé›†=å‰217ä¸ªæ ·æœ¬ï¼ŒéªŒè¯é›†=ç¬¬218-272ä¸ªæ ·æœ¬ï¼ˆæ— é‡å ï¼‰
5. ç»“æœä¿å­˜ï¼šä¿å­˜æ‰€æœ‰æ ·æœ¬çš„é€‰ä¸­ç‰¹å¾åˆ—ï¼ˆæ— è¡Œç¼ºå¤±ï¼‰
-------------------------------------------------------------
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind
from sklearn.feature_selection import mutual_info_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score

# ------------------- å…¨å±€å‚æ•°ï¼ˆå¯æŒ‰éœ€è°ƒæ•´ï¼‰ -------------------
VOWELS = ["A", "E", "I", "O", "U", "KA", "PA", "TA"]
TT_DIR = "/home/oem/qinglan7/ICA/DATA/tt/MEL/ViT/"
TEMPLATE_FILE = "A.csv"  # ç‰¹å¾æ–‡ä»¶æ¨¡æ¿ï¼ˆæ›¿æ¢Aä¸ºç›®æ ‡å‘éŸ³ï¼‰
DATA_CSV = "/home/oem/qinglan7/ICA/DATA/tt/DATA.CSV/Vit.csv"  # æ ‡ç­¾æ–‡ä»¶è·¯å¾„
OUT_DIR = "/home/oem/qinglan7/ICA/DATA/tt/MEL/ViT/select/"
os.makedirs(OUT_DIR, exist_ok=True)  # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨

# ç‰¹å¾é€‰æ‹©ä¸æ¨¡å‹å‚æ•°
P_VALUE_TH = 0.01  # t-test æ˜¾è‘—æ€§é˜ˆå€¼
MI_KEEP_LIST = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]  # å¾…æµ‹è¯•çš„MIç‰¹å¾æ•°
RANDOM_STATE = 0  # å›ºå®šéšæœºç§å­ï¼ˆå¯å¤ç°ï¼‰
N_ESTIMATORS = 200  # éšæœºæ£®æ—æ ‘æ•°é‡
TRAIN_SAMPLE_NUM = 217  # è®­ç»ƒé›†ï¼šå‰217ä¸ªæ ·æœ¬ï¼ˆ1-217ï¼‰
VAL_SAMPLE_NUM = 55  # éªŒè¯é›†ï¼š218-272ä¸ªæ ·æœ¬ï¼ˆå…±55ä¸ªï¼‰
VAL_START_IDX = TRAIN_SAMPLE_NUM  # éªŒè¯é›†èµ·å§‹ç´¢å¼•ï¼ˆ217ï¼‰
VAL_END_IDX = TRAIN_SAMPLE_NUM + VAL_SAMPLE_NUM  # éªŒè¯é›†ç»“æŸç´¢å¼•ï¼ˆ272ï¼‰
VAR_THRESHOLD = 1e-8  # æ–¹å·®é˜ˆå€¼ï¼ˆå°äºè¯¥å€¼è§†ä¸ºæ’å®šç‰¹å¾ï¼‰

# ç»“æœå­˜å‚¨å®¹å™¨
summary = []
all_results = []


# ------------------- å·¥å…·å‡½æ•°ï¼ˆå¢å¼ºå¤ç”¨æ€§ï¼‰ -------------------
def filter_constant_features(X, feat_names, var_threshold=VAR_THRESHOLD):
    """è¿‡æ»¤æ’å®šç‰¹å¾ï¼ˆæ–¹å·®æ¥è¿‘0ï¼‰"""
    var = X.var(axis=0)  # è®¡ç®—æ¯åˆ—æ–¹å·®
    non_constant_mask = var > var_threshold
    X_filtered = X[:, non_constant_mask]
    feat_names_filtered = feat_names[non_constant_mask]
    removed_feats = feat_names[~non_constant_mask]
    if len(removed_feats) > 0:
        print(f"âš ï¸ ç§»é™¤ {len(removed_feats)} ä¸ªæ’å®šç‰¹å¾ï¼ˆæ–¹å·®â‰ˆ0ï¼‰")
    return X_filtered, feat_names_filtered


# ------------------- ä¸»æµç¨‹ -------------------
if __name__ == "__main__":
    # è¯»å–æ‰€æœ‰æ ·æœ¬çš„æ ‡ç­¾ï¼ˆID + Classï¼‰
    try:
        df_label_all = pd.read_csv(DATA_CSV)
        assert "ID" in df_label_all.columns and "Class" in df_label_all.columns, \
            "DATA.csv å¿…é¡»åŒ…å« 'ID' å’Œ 'Class' åˆ—"
        assert len(df_label_all) >= VAL_END_IDX, \
            f"æ ‡ç­¾æ–‡ä»¶æ ·æœ¬æ•°ä¸è¶³ {VAL_END_IDX} ä¸ªï¼ˆå½“å‰ä»… {len(df_label_all)} ä¸ªï¼‰"
    except Exception as e:
        raise ValueError(f"è¯»å–æ ‡ç­¾æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")

    for vowel in VOWELS:
        # æ„å»ºå½“å‰å‘éŸ³çš„ç‰¹å¾æ–‡ä»¶è·¯å¾„
        file_path = os.path.join(TT_DIR, TEMPLATE_FILE.replace("A", vowel))
        if not os.path.exists(file_path):
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}ï¼Œè·³è¿‡è¯¥å‘éŸ³")
            continue

        print(f"\n========================= ğŸ”¤ å¤„ç†å‘éŸ³: {vowel} =========================")
        # è¯»å–å½“å‰å‘éŸ³çš„æ‰€æœ‰æ ·æœ¬ç‰¹å¾
        try:
            df_feat_all = pd.read_csv(file_path)
            assert "ID" in df_feat_all.columns, f"{file_path} å¿…é¡»åŒ…å« 'ID' åˆ—"
        except Exception as e:
            print(f"âŒ è¯»å–ç‰¹å¾æ–‡ä»¶å¤±è´¥ï¼š{str(e)}ï¼Œè·³è¿‡è¯¥å‘éŸ³")
            continue

        # ------------------- å…³é”®ä¿®å¤1ï¼šåˆ†ç¦»IDåˆ—å’Œæ•°å€¼ç‰¹å¾åˆ—ï¼ˆæ˜ç¡®æ’é™¤IDï¼‰ -------------------
        # æ•°å€¼ç‰¹å¾åˆ—ï¼šæ‰€æœ‰æ•°å€¼ç±»å‹åˆ—ï¼Œä¸”æ’é™¤IDåˆ—ï¼ˆé¿å…åç»­ç´¢å¼•æ“ä½œåæ‰¾ä¸åˆ°åˆ—ï¼‰
        feat_cols_all = df_feat_all.select_dtypes(include=[np.number]).columns.tolist()
        if "ID" in feat_cols_all:
            feat_cols_all.remove("ID")  # ç¡®ä¿æ•°å€¼ç‰¹å¾åˆ—ä¸­ä¸å«ID
        feat_cols_all = np.array(feat_cols_all)  # è½¬æˆæ•°ç»„æ–¹ä¾¿åç»­ç´¢å¼•
        print(f"åŸå§‹ç‰¹å¾æ€»æ•°ï¼š{len(feat_cols_all)}")

        # ------------------- æ­¥éª¤1ï¼šåˆ’åˆ†å›ºå®šè®­ç»ƒé›†/éªŒè¯é›†ï¼ˆæ— é‡å ï¼‰ -------------------
        # è®­ç»ƒé›†ï¼šå‰217ä¸ªæ ·æœ¬ï¼ˆ1-217ï¼‰
        df_train_label = df_label_all.iloc[:TRAIN_SAMPLE_NUM].copy()
        train_ids = df_train_label["ID"].values
        print(f"è®­ç»ƒé›†ï¼š{len(train_ids)} ä¸ªæ ·æœ¬ï¼ˆIDï¼š{train_ids[:3]}...{train_ids[-3:]}ï¼‰")

        # éªŒè¯é›†ï¼šç¬¬218-272ä¸ªæ ·æœ¬ï¼ˆå…±55ä¸ªï¼‰
        df_val_label = df_label_all.iloc[VAL_START_IDX:VAL_END_IDX].copy()
        val_ids = df_val_label["ID"].values
        print(f"éªŒè¯é›†ï¼š{len(val_ids)} ä¸ªæ ·æœ¬ï¼ˆIDï¼š{val_ids[:3]}...{val_ids[-3:]}ï¼‰")

        # ------------------- æ­¥éª¤2ï¼št-test ç‰¹å¾é€‰æ‹©ï¼ˆä»…ç”¨è®­ç»ƒé›†ï¼šå‰217ä¸ªæ ·æœ¬ï¼‰ -------------------
        # å¯¹é½è®­ç»ƒé›†ç‰¹å¾ï¼ˆä¿è¯IDé¡ºåºä¸€è‡´ï¼‰
        df_feat_train = df_feat_all[df_feat_all["ID"].isin(train_ids)].copy()  # ä¸æå‰è®¾ç´¢å¼•
        df_feat_train = df_feat_train.set_index("ID").reindex(train_ids).reset_index()  # å…ˆå¯¹é½å†é‡ç½®ç´¢å¼•

        # ------------------- å…³é”®ä¿®å¤2ï¼šæå‰åˆå§‹åŒ– valid_train_idsï¼ˆé¿å…æœªå®šä¹‰ï¼‰ -------------------
        valid_train_ids = df_feat_train["ID"].values  # é»˜è®¤ä½¿ç”¨æ‰€æœ‰è®­ç»ƒé›†æ ·æœ¬ID
        if df_feat_train.isnull().any().any():
            print(f"âš ï¸ è®­ç»ƒé›†ä¸­å­˜åœ¨ç¼ºå¤±çš„ç‰¹å¾æ•°æ®ï¼Œå·²è‡ªåŠ¨åˆ é™¤å«NaNçš„è¡Œ")
            df_feat_train = df_feat_train.dropna()
            # æ›´æ–°æœ‰æ•ˆè®­ç»ƒé›†IDï¼ˆä»…ä¿ç•™ç‰¹å¾éç©ºçš„æ ·æœ¬ï¼‰
            valid_train_ids = df_feat_train["ID"].values
            # åŒæ­¥æ›´æ–°è®­ç»ƒé›†æ ‡ç­¾
            df_train_label = df_train_label[df_train_label["ID"].isin(valid_train_ids)]

        # æå–t-testç”¨çš„ç‰¹å¾çŸ©é˜µå’Œæ ‡ç­¾ï¼ˆæ­¤æ—¶df_feat_trainä»æœ‰IDåˆ—ï¼Œéœ€æ’é™¤ï¼‰
        X_t_train = df_feat_train[feat_cols_all].values
        y_t_train = df_train_label.set_index("ID").loc[valid_train_ids]["Class"].astype(float).values

        # æ‰§è¡Œt-testï¼ˆå¤šç±»åˆ«ï¼šæ¯ä¸ªç±»åˆ«ä¸å…¶ä»–ç±»åˆ«å¯¹æ¯”ï¼‰
        mask_t = np.zeros(X_t_train.shape[1], dtype=bool)
        unique_cls = np.unique(y_t_train[~np.isnan(y_t_train)])  # æ’é™¤NaNæ ‡ç­¾
        if len(unique_cls) < 2:
            print(f"âš ï¸ è®­ç»ƒé›†ä¸­ä»…åŒ…å« {len(unique_cls)} ä¸ªç±»åˆ«ï¼Œè·³è¿‡t-testï¼Œä½¿ç”¨æ‰€æœ‰ç‰¹å¾")
            mask_t = np.ones(X_t_train.shape[1], dtype=bool)
        else:
            for cls in unique_cls:
                cls_mask = (y_t_train == cls) & (~np.isnan(y_t_train))
                other_mask = (~cls_mask) & (~np.isnan(y_t_train))
                # æ‰§è¡Œç‹¬ç«‹æ ·æœ¬t-testï¼ˆä¸å‡è®¾æ–¹å·®ç›¸ç­‰ï¼‰
                _, p_vals = ttest_ind(
                    X_t_train[cls_mask], X_t_train[other_mask],
                    axis=0, equal_var=False, nan_policy='omit'
                )
                mask_t |= (p_vals <= P_VALUE_TH)  # åªè¦ä¸€ä¸ªç±»åˆ«æ»¡è¶³å°±ä¿ç•™ç‰¹å¾

        # åº”ç”¨t-testç­›é€‰ç»“æœï¼ˆæ‰€æœ‰æ ·æœ¬éƒ½ä¿ç•™ç­›é€‰åçš„ç‰¹å¾åˆ—ï¼‰
        feat_t = feat_cols_all[mask_t]
        X_t_all = df_feat_all[feat_t].values  # æ‰€æœ‰æ ·æœ¬çš„t-teståç‰¹å¾ï¼ˆdf_feat_allæœªæ”¹ç´¢å¼•ï¼Œç›´æ¥é€‰ï¼‰
        print(f"t-test ç­›é€‰åç‰¹å¾æ•°ï¼š{len(feat_t)}")

        # ------------------- æ­¥éª¤3ï¼šè¿‡æ»¤æ’å®šç‰¹å¾ï¼ˆæ–¹å·®â‰ˆ0ï¼‰ -------------------
        X_t_filtered, feat_t_filtered = filter_constant_features(X_t_all, feat_t)
        if len(feat_t_filtered) == 0:
            print(f"âŒ t-teståæ— æœ‰æ•ˆç‰¹å¾ï¼ˆå…¨éƒ¨ä¸ºæ’å®šç‰¹å¾ï¼‰ï¼Œè·³è¿‡è¯¥å‘éŸ³")
            continue
        print(f"è¿‡æ»¤æ’å®šç‰¹å¾åå‰©ä½™ï¼š{len(feat_t_filtered)} ä¸ªç‰¹å¾")

        # ------------------- æ­¥éª¤4ï¼šMI ç‰¹å¾é€‰æ‹©ï¼ˆä»…ç”¨è®­ç»ƒé›†ï¼šå‰217ä¸ªæ ·æœ¬ï¼‰ -------------------
        # æå–è®­ç»ƒé›†çš„è¿‡æ»¤åç‰¹å¾ï¼ˆç”¨äºMIè®¡ç®—ï¼‰
        X_mi_train = df_feat_train[feat_t_filtered].values
        y_mi_train = df_train_label["Class"].astype(int).values

        # ------------------- æ­¥éª¤5ï¼šå¯¹é½éªŒè¯é›†ç‰¹å¾ï¼ˆç¡®ä¿æ— é‡å ã€æ— ç¼ºå¤±ï¼‰ -------------------
        # å¯¹é½éªŒè¯é›†ç‰¹å¾ï¼ˆæŒ‰éªŒè¯é›†IDé¡ºåºæ’åˆ—ï¼‰
        df_feat_val = df_feat_all[df_feat_all["ID"].isin(val_ids)].copy()
        df_feat_val = df_feat_val.set_index("ID").reindex(val_ids).reset_index()

        # æå‰åˆå§‹åŒ– valid_val_idsï¼ˆé¿å…æœªå®šä¹‰ï¼‰
        valid_val_ids = df_feat_val["ID"].values
        if df_feat_val.isnull().any().any():
            print(f"âš ï¸ éªŒè¯é›†ä¸­å­˜åœ¨ç¼ºå¤±çš„ç‰¹å¾æ•°æ®ï¼Œå·²è‡ªåŠ¨åˆ é™¤å«NaNçš„è¡Œ")
            df_feat_val = df_feat_val.dropna()
            # æ›´æ–°æœ‰æ•ˆéªŒè¯é›†ID
            valid_val_ids = df_feat_val["ID"].values
            # åŒæ­¥æ›´æ–°éªŒè¯é›†æ ‡ç­¾
            df_val_label = df_val_label[df_val_label["ID"].isin(valid_val_ids)]

        # æå–éªŒè¯é›†çš„è¿‡æ»¤åç‰¹å¾ï¼ˆç”¨äºæ¨¡å‹è¯„ä¼°ï¼‰
        X_val = df_feat_val[feat_t_filtered].values
        y_val = df_val_label.set_index("ID").loc[valid_val_ids]["Class"].astype(int).values
        print(f"æœ‰æ•ˆéªŒè¯é›†æ ·æœ¬æ•°ï¼š{len(y_val)}ï¼ˆåŸå§‹55ä¸ªï¼‰")

        # éå†ä¸åŒçš„MIç‰¹å¾ä¿ç•™æ•°ï¼Œè¯„ä¼°æ¨¡å‹æ€§èƒ½
        results = []
        for mi_keep in MI_KEEP_LIST:
            # è®¡ç®—äº’ä¿¡æ¯åˆ†æ•°ï¼ˆä»…ç”¨è®­ç»ƒé›†ï¼‰
            mi_scores = mutual_info_classif(
                X_mi_train, y_mi_train,
                discrete_features=False,
                n_neighbors=5,  # å‡å°‘æ•°å€¼è¯¯å·®
                random_state=RANDOM_STATE
            )
            # é€‰æ‹©MIåˆ†æ•°æœ€é«˜çš„å‰Nä¸ªç‰¹å¾ï¼ˆé¿å…è¶…è¿‡å¯ç”¨ç‰¹å¾æ•°ï¼‰
            select_num = min(mi_keep, len(mi_scores))
            top_idx = np.argsort(mi_scores)[-select_num:]  # å€’åºæ’åºï¼Œå–Top N
            feat_final = list(feat_t_filtered[top_idx])  # æœ€ç»ˆé€‰ä¸­çš„ç‰¹å¾åç§°

            # ------------------- æ­¥éª¤6ï¼šä¿å­˜æ‰€æœ‰æ ·æœ¬çš„é€‰ä¸­ç‰¹å¾ -------------------
            X_final_all = X_t_filtered[:, top_idx]  # æ‰€æœ‰æ ·æœ¬çš„æœ€ç»ˆç‰¹å¾
            df_final = pd.DataFrame(X_final_all, columns=feat_final)
            df_final["ID"] = df_feat_all["ID"].values  # æ‰€æœ‰æ ·æœ¬çš„ID
            # åŒ¹é…Classæ ‡ç­¾ï¼ˆå¤„ç†IDä¸åŒ¹é…ï¼‰
            id_to_class = dict(zip(df_label_all["ID"], df_label_all["Class"]))
            df_final["Class"] = df_final["ID"].map(id_to_class)
            df_final = df_final[["ID", "Class"] + feat_final]

            # ä¿å­˜æ–‡ä»¶
            out_file = os.path.join(OUT_DIR, f"{vowel}_{mi_keep}_selectedfeatures_ALL_SAMPLES.csv")
            df_final.to_csv(out_file, index=False, encoding="utf-8")
            print(f"ğŸ“Š å·²ä¿å­˜ï¼š{os.path.basename(out_file)}ï¼ˆæ ·æœ¬æ•°ï¼š{len(df_final)}ï¼‰")

            # ------------------- æ­¥éª¤7ï¼šéšæœºæ£®æ—è®­ç»ƒä¸è¯„ä¼°ï¼ˆè®­ç»ƒé›†â†’éªŒè¯é›†ï¼‰ -------------------
            # è®­ç»ƒé›†ç‰¹å¾ï¼ˆMIç­›é€‰åï¼‰
            X_train = X_mi_train[:, top_idx]
            # éªŒè¯é›†ç‰¹å¾ï¼ˆMIç­›é€‰åï¼‰
            X_val_final = X_val[:, top_idx]

            # è®­ç»ƒéšæœºæ£®æ—ï¼ˆä»…ç”¨è®­ç»ƒé›†ï¼‰
            model = RandomForestClassifier(
                n_estimators=N_ESTIMATORS,
                random_state=RANDOM_STATE,
                class_weight="balanced",
                n_jobs=-1  # å¹¶è¡Œè®­ç»ƒ
            )
            model.fit(X_train, y_mi_train)  # ä»…ç”¨å‰217ä¸ªæ ·æœ¬è®­ç»ƒ

            # éªŒè¯é›†é¢„æµ‹ï¼ˆä»…ç”¨ç¬¬218-272ä¸ªæ ·æœ¬ï¼‰
            y_pred = model.predict(X_val_final)

            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            acc = accuracy_score(y_val, y_pred)
            f1 = f1_score(y_val, y_pred, average="macro")  # å¤šç±»åˆ«ç”¨macro-F1

            # è®°å½•ç»“æœ
            print(f"MI_KEEP={mi_keep:<4} â†’ Accuracy={acc:.4f} | Macro-F1={f1:.4f}")
            results.append((mi_keep, acc, f1))
            all_results.append({
                "Vowel": vowel,
                "MI_KEEP": mi_keep,
                "Accuracy": round(acc, 4),
                "Macro_F1": round(f1, 4),
                "Train_Samples": len(X_train),
                "Val_Samples": len(X_val_final)
            })

        # ------------------- æ­¥éª¤8ï¼šè®°å½•å½“å‰å‘éŸ³çš„æœ€ä½³ç»“æœ -------------------
        if results:
            best_mi_keep, best_acc, best_f1 = max(results, key=lambda x: x[1])  # æŒ‰å‡†ç¡®ç‡é€‰æœ€ä½³
            summary.append((vowel, best_mi_keep, best_acc, best_f1))
            print(f"ğŸ† {vowel} æœ€ä½³ç»“æœï¼šMI_KEEP={best_mi_keep} â†’ Acc={best_acc:.4f}, F1={best_f1:.4f}")

        # ------------------- æ­¥éª¤9ï¼šç»˜åˆ¶å•å‘éŸ³æ€§èƒ½æŠ˜çº¿å›¾ -------------------
        df_plot = pd.DataFrame(results, columns=["MI_KEEP", "Accuracy", "Macro_F1"])
        plt.figure(figsize=(8, 5))
        plt.plot(df_plot["MI_KEEP"], df_plot["Accuracy"], marker="o", linewidth=2, label="Accuracy")
        plt.plot(df_plot["MI_KEEP"], df_plot["Macro_F1"], marker="s", linewidth=2, label="Macro-F1")
        plt.title(f"{vowel}ï¼šPerformance vs MI Feature Count", fontsize=12)
        plt.xlabel("Number of MI Selected Features", fontsize=10)
        plt.ylabel("Score", fontsize=10)
        plt.grid(True, linestyle="--", alpha=0.6)
        plt.legend(fontsize=10)
        plt.xticks(df_plot["MI_KEEP"], rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(OUT_DIR, f"{vowel}_performance_curve.png"), dpi=300)
        plt.close()
    # ------------------- æœ€ç»ˆæ±‡æ€» -------------------
    # ä¿å­˜æ‰€æœ‰ç»“æœåˆ°CSV
    df_all_results = pd.DataFrame(all_results).sort_values(["Vowel", "MI_KEEP"])
    df_all_results.to_csv(os.path.join(OUT_DIR, "all_vowels_detailed_results.csv"), index=False, encoding="utf-8")

    # ä¿å­˜æœ€ä½³ç»“æœæ±‡æ€»
    df_summary = pd.DataFrame(summary, columns=["Vowel", "Best_MI_KEEP", "Best_Accuracy", "Best_Macro_F1"])
    df_summary.to_csv(os.path.join(OUT_DIR, "all_vowels_best_results.csv"), index=False, encoding="utf-8")

    # æ‰“å°æœ€ä½³ç»“æœæ±‡æ€»
    print("\n" + "="*50)
    print("æ‰€æœ‰å‘éŸ³çš„æœ€ä½³ç»“æœæ±‡æ€»ï¼ˆè®­ç»ƒé›†=1-217ï¼ŒéªŒè¯é›†=218-272ï¼‰")
    print("="*50)
    print(df_summary.to_string(index=False))

    # ------------------- ç»˜åˆ¶æ‰€æœ‰å‘éŸ³çš„æ±‡æ€»æŠ˜çº¿å›¾ -------------------
    # å‡†ç¡®ç‡æ±‡æ€»å›¾
    plt.figure(figsize=(12, 6))
    for vowel in VOWELS:
        sub_data = df_all_results[df_all_results["Vowel"] == vowel]
        if not sub_data.empty:
            plt.plot(sub_data["MI_KEEP"], sub_data["Accuracy"], marker="o", linewidth=2, label=vowel)
    plt.title("All Vowels: Accuracy vs MI Feature Count", fontsize=14)
    plt.xlabel("Number of MI Selected Features", fontsize=12)
    plt.ylabel("Accuracy", fontsize=12)
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.legend(title="Vowel", bbox_to_anchor=(1.05, 1), loc="upper left", fontsize=10)
    plt.tight_layout()
    plt.savefig(os.path.join(OUT_DIR, "all_vowels_accuracy_summary.png"), dpi=300)
    plt.close()

    # Macro-F1æ±‡æ€»å›¾
    plt.figure(figsize=(12, 6))
    for vowel in VOWELS:
        sub_data = df_all_results[df_all_results["Vowel"] == vowel]
        if not sub_data.empty:
            plt.plot(sub_data["MI_KEEP"], sub_data["Macro_F1"], marker="o", linewidth=2, label=vowel)
    #plt.title("All Vowels: Macro-F1 vs MI Feature Count", fontsize=14)
    plt.xlabel("K", fontsize=12)
    plt.ylabel("Avg.F1-score", fontsize=12)
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.legend(title="Vowel", bbox_to_anchor=(1.05, 1), loc="upper left", fontsize=10)
    plt.tight_layout()
    plt.savefig(os.path.join(OUT_DIR, "all_vowels_macrof1_summary.png"), dpi=300)
    plt.close()

    print(f"\nâœ… æ‰€æœ‰å‘éŸ³å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ï¼š{OUT_DIR}")